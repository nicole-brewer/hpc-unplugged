# Background

Computers are everywhere. Smart phones for instance, are a lot smaller than they used to be because the computers inside them got a lot smaller as engineers improved their design. In recent years, however, we have reached the physical limitations of how small a computer can be before it starts to melt when it gets used. So now instead of trying to make computers more smaller and faster, engineers build supercomputers which essentially are a lot of computers that work together to solve a task.

Engineers might use supercomputers to simulate the aerodynamics of a fighter jet. Scientists use supercomputers to forecast the weather. Social scientists use supercomputers to analyse connections between users on Instagram. But using a supercomputer is not like using a desktop computer. Because supercomputers are made of of lot of smaller computers connected over a network, scientists have to learn how to break up their problem into smaller pieces and then integrate the results back into a larger picture. This lesson demonstrates why sometimes can be straight forward, and other times slows down the process to some degree.

## Concepts 

Parallelism: Using additional computational resources simultaneously, usually for speedup [CS13]

Distributed computing: Using additional computers simultaneously together over a network

Performance: Measuring and tuning metrics - typically computer memory, time, and energy - to achieve speedup, efficiency, and scalability.

